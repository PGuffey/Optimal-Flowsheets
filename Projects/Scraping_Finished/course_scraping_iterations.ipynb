{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to slu_courses.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import mechanize\n",
    "\n",
    "# Initialize mechanize browser\n",
    "br = mechanize.Browser()\n",
    "br.set_handle_equiv(True)\n",
    "br.set_handle_redirect(True)\n",
    "br.set_handle_referer(True)\n",
    "br.set_handle_robots(False)\n",
    "\n",
    "# Open a specific course page (Computer Science in this example)\n",
    "link = \"https://catalog.slu.edu/courses-az/csci/\"\n",
    "br.open(link)\n",
    "html = br.response().read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find all course blocks\n",
    "course_blocks = soup.find_all(\"div\", class_=\"courseblock\")\n",
    "\n",
    "# Initialize list to store course data\n",
    "courses_data = []\n",
    "\n",
    "# Extract data from each course block\n",
    "for block in course_blocks:\n",
    "    course_title = block.find(\"p\", class_=\"courseblocktitle noindent\")\n",
    "    credits = block.find(\"p\", class_=\"courseblockextra noindent\")\n",
    "    prerequisites = block.find(\"p\", class_=\"courseblockprereq noindent\")\n",
    "\n",
    "    courses_data.append({\n",
    "        'Course Title': course_title.text.strip() if course_title else 'NaN',\n",
    "        'Credits': credits.text.strip() if credits else 'NaN',\n",
    "        'Prerequisites': prerequisites.text.strip() if prerequisites else 'NaN'\n",
    "    })\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "courses_df = pd.DataFrame(courses_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = \"slu_courses.csv\"\n",
    "courses_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to slu_courses2.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import mechanize\n",
    "\n",
    "# Initialize mechanize browser\n",
    "br = mechanize.Browser()\n",
    "br.set_handle_equiv(True)\n",
    "br.set_handle_redirect(True)\n",
    "br.set_handle_referer(True)\n",
    "br.set_handle_robots(False)\n",
    "\n",
    "# Open a specific course page (Computer Science in this example)\n",
    "link = \"https://catalog.slu.edu/courses-az/csci/\"\n",
    "br.open(link)\n",
    "html = br.response().read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find all course blocks\n",
    "course_blocks = soup.find_all(\"div\", class_=\"courseblock\")\n",
    "\n",
    "# Initialize list to store course data\n",
    "courses_data = []\n",
    "\n",
    "# Extract data from each course block\n",
    "for block in course_blocks:\n",
    "    course_title = block.find(\"p\", class_=\"courseblocktitle noindent\")\n",
    "    credits = block.find(\"p\", class_=\"courseblockextra noindent\")\n",
    "    \n",
    "    # Find prerequisite information\n",
    "    prerequisites = \"NaN\"\n",
    "    for extra in block.find_all(\"p\", class_=\"courseblockextra noindent\"):\n",
    "        if extra.find(\"strong\") and \"Prerequisite(s):\" in extra.find(\"strong\").text:\n",
    "            prereq_links = extra.find_all(\"a\")\n",
    "            prereqs = [link.text.strip() for link in prereq_links]\n",
    "            prerequisites = ', '.join(prereqs)\n",
    "            break\n",
    "\n",
    "    courses_data.append({\n",
    "        'Course Title': course_title.text.strip() if course_title else 'NaN',\n",
    "        'Credits': credits.text.strip() if credits else 'NaN',\n",
    "        'Prerequisites': prerequisites\n",
    "    })\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "courses_df = pd.DataFrame(courses_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = \"slu_courses2.csv\"\n",
    "courses_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to slu_courses.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import mechanize\n",
    "\n",
    "# Initialize mechanize browser\n",
    "br = mechanize.Browser()\n",
    "br.set_handle_equiv(True)\n",
    "br.set_handle_redirect(True)\n",
    "br.set_handle_referer(True)\n",
    "br.set_handle_robots(False)\n",
    "\n",
    "# Open a specific course page (Computer Science in this example)\n",
    "link = \"https://catalog.slu.edu/courses-az/csci/\"\n",
    "br.open(link)\n",
    "html = br.response().read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find all course blocks\n",
    "course_blocks = soup.find_all(\"div\", class_=\"courseblock\")\n",
    "\n",
    "# Initialize list to store course data\n",
    "courses_data = []\n",
    "\n",
    "# Extract data from each course block\n",
    "for block in course_blocks:\n",
    "    course_title = block.find(\"p\", class_=\"courseblocktitle noindent\")\n",
    "    credits = block.find(\"p\", class_=\"courseblockextra noindent\")\n",
    "    \n",
    "    # Find prerequisite information\n",
    "    prerequisites = \"NaN\"\n",
    "    concurrent_enrollment = \"NaN\"\n",
    "    for extra in block.find_all(\"p\", class_=\"courseblockextra noindent\"):\n",
    "        if extra.find(\"strong\") and \"Prerequisite(s):\" in extra.find(\"strong\").text:\n",
    "            prereq_links = extra.find_all(\"a\")\n",
    "            prereqs = [link.text.strip() for link in prereq_links]\n",
    "            prerequisites = ', '.join(prereqs)\n",
    "            \n",
    "            # Check for concurrent enrollment info\n",
    "            sup_tags = extra.find_all(\"sup\")\n",
    "            for sup in sup_tags:\n",
    "                if \"Concurrent enrollment allowed.\" in sup.next_sibling:\n",
    "                    concurrent_enrollment = \"Concurrent enrollment allowed.\"\n",
    "                    break\n",
    "            break\n",
    "\n",
    "    courses_data.append({\n",
    "        'Course Title': course_title.text.strip() if course_title else 'NaN',\n",
    "        'Credits': credits.text.strip() if credits else 'NaN',\n",
    "        'Prerequisites': prerequisites,\n",
    "        'Concurrent Enrollment': concurrent_enrollment\n",
    "    })\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "courses_df = pd.DataFrame(courses_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = \"slu_courses.csv\"\n",
    "courses_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "httperror_seek_wrapper",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mhttperror_seek_wrapper\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m     69\u001b[0m     major_link, major_name \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m     major_courses_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_course_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmajor_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Add major name to each course entry\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m course \u001b[38;5;129;01min\u001b[39;00m major_courses_data:\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mscrape_course_data\u001b[1;34m(major_link)\u001b[0m\n\u001b[0;32m     15\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://catalog.slu.edu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m link \u001b[38;5;241m=\u001b[39m base_url \u001b[38;5;241m+\u001b[39m major_link\n\u001b[1;32m---> 17\u001b[0m \u001b[43mbr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m html \u001b[38;5;241m=\u001b[39m br\u001b[38;5;241m.\u001b[39mresponse()\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     19\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\guffe\\Desktop\\Computer_Science\\.venv\\Lib\\site-packages\\mechanize\\_mechanize.py:257\u001b[0m, in \u001b[0;36mBrowser.open\u001b[1;34m(self, url_or_request, data, timeout)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    245\u001b[0m          url_or_request,\n\u001b[0;32m    246\u001b[0m          data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    247\u001b[0m          timeout\u001b[38;5;241m=\u001b[39m_sockettimeout\u001b[38;5;241m.\u001b[39m_GLOBAL_DEFAULT_TIMEOUT):\n\u001b[0;32m    248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m    Open a URL. Loads the page so that you can subsequently use\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    :meth:`forms()`, :meth:`links()`, etc. on it.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    :return: A :class:`mechanize.Response` object\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mech_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guffe\\Desktop\\Computer_Science\\.venv\\Lib\\site-packages\\mechanize\\_mechanize.py:313\u001b[0m, in \u001b[0;36mBrowser._mech_open\u001b[1;34m(self, url, data, update_history, visit, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m     response \u001b[38;5;241m=\u001b[39m _response\u001b[38;5;241m.\u001b[39mupgrade_response(response)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m response\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mhttperror_seek_wrapper\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import mechanize\n",
    "\n",
    "# Function to scrape course data for a given link\n",
    "def scrape_course_data(major_link):\n",
    "    # Initialize mechanize browser\n",
    "    br = mechanize.Browser()\n",
    "    br.set_handle_equiv(True)\n",
    "    br.set_handle_redirect(True)\n",
    "    br.set_handle_referer(True)\n",
    "    br.set_handle_robots(False)\n",
    "\n",
    "    # Open the course page\n",
    "    base_url = \"https://catalog.slu.edu\"\n",
    "    link = base_url + major_link\n",
    "    br.open(link)\n",
    "    html = br.response().read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Find all course blocks\n",
    "    course_blocks = soup.find_all(\"div\", class_=\"courseblock\")\n",
    "\n",
    "    # Initialize list to store course data\n",
    "    courses_data = []\n",
    "\n",
    "    # Extract data from each course block\n",
    "    for block in course_blocks:\n",
    "        course_title = block.find(\"p\", class_=\"courseblocktitle noindent\")\n",
    "        credits = block.find(\"p\", class_=\"courseblockextra noindent\")\n",
    "        \n",
    "        # Find prerequisite information\n",
    "        prerequisites = \"NaN\"\n",
    "        concurrent_enrollment = \"NaN\"\n",
    "\n",
    "        for extra in block.find_all(\"p\", class_=\"courseblockextra noindent\"):\n",
    "            if extra.find(\"strong\") and \"Prerequisite(s):\" in extra.find(\"strong\").text:\n",
    "                prereq_links = extra.find_all(\"a\")\n",
    "                prereqs = [link.text.strip() for link in prereq_links]\n",
    "                prerequisites = ', '.join(prereqs)\n",
    "                \n",
    "                # Check for concurrent enrollment info\n",
    "                sup_tags = extra.find_all(\"sup\")\n",
    "                for sup in sup_tags:\n",
    "                    if sup.next_sibling and \"Concurrent enrollment allowed.\" in sup.next_sibling:\n",
    "                        concurrent_enrollment = \"Concurrent enrollment allowed.\"\n",
    "                        break\n",
    "                break\n",
    "\n",
    "        courses_data.append({\n",
    "            'Course Title': course_title.text.strip() if course_title else 'NaN',\n",
    "            'Credits': credits.text.strip() if credits else 'NaN',\n",
    "            'Prerequisites': prerequisites,\n",
    "            'Concurrent Enrollment': concurrent_enrollment\n",
    "        })\n",
    "\n",
    "    return courses_data\n",
    "\n",
    "# Read the major links from the file\n",
    "major_links_file = '../course_links.txt'\n",
    "with open(major_links_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize a list to store all courses data\n",
    "all_courses_data = []\n",
    "\n",
    "# Process each major link\n",
    "for line in lines:\n",
    "    major_link, major_name = line.strip().split(', ', 1)\n",
    "    major_courses_data = scrape_course_data(major_link)\n",
    "    \n",
    "    # Add major name to each course entry\n",
    "    for course in major_courses_data:\n",
    "        course['Major'] = major_name\n",
    "    \n",
    "    all_courses_data.extend(major_courses_data)\n",
    "\n",
    "# Convert the combined data to a DataFrame\n",
    "all_courses_df = pd.DataFrame(all_courses_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = \"all_slu_courses4.csv\"\n",
    "all_courses_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
